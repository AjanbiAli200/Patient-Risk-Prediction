{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9c4228",
   "metadata": {},
   "source": [
    "**==============================================================**\n",
    "#### **‚úÖ Retrieval-Augmented Chatbot Code**\n",
    "#### **RAG Chatbot for Patient Risk Prediction Project**\n",
    "**==============================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c2109",
   "metadata": {},
   "source": [
    "#### **üß± 1Ô∏è‚É£ Import Libraries & Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35327f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# üß† Step 1: Import all necessary libraries\n",
    "# -----------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from opensearchpy import OpenSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4fc8f",
   "metadata": {},
   "source": [
    "#### **==============================================================**\n",
    "#### **‚öôÔ∏è Configuration**\n",
    "#### **==============================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "642c0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"patient_risk_docs\"\n",
    "EMBED_CACHE_PATH = r\"D:\\Patient Risk Prediction\\Patient-Risk-Prediction\\chatbot\\cache\\embeddings_cache.json\"\n",
    "MODEL_NAME = \"BAAI/bge-small-en\"\n",
    "OPENSEARCH_HOST = \"localhost\"\n",
    "PORT = 9200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00737358",
   "metadata": {},
   "source": [
    "#### **üß† 2 Configure Databricks Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30c40546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import sql\n",
    "\n",
    "# üîê Databricks Connection Config\n",
    "DATABRICKS_CONFIG = {\n",
    "    \"server_hostname\": \"XXXXXX-e088.cloud.databricks.com\",  # replace with yours\n",
    "    \"http_path\": \"/sql/1.0/warehouses/XXXXXXXXXXXXX\",             # from SQL warehouse settings\n",
    "    \"access_token\": \"XXXXXXXXXXXXXX\"             # your PAT\n",
    "}\n",
    "\n",
    "def run_databricks_query(query):\n",
    "    try:\n",
    "        with sql.connect(**DATABRICKS_CONFIG) as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(query)\n",
    "                result = cursor.fetchall()\n",
    "                columns = [desc[0] for desc in cursor.description]\n",
    "                return {\"columns\": columns, \"rows\": result}\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Databricks query error:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3feae2",
   "metadata": {},
   "source": [
    "#### **‚öôÔ∏è 2Ô∏è‚É£ OpenSearch Connection Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0e201f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to OpenSearch 2.9.0\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# ‚öôÔ∏è Step 2: Connect to your local OpenSearch instance\n",
    "# -----------------------------------------------------\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": PORT}],\n",
    "    http_auth=(\"admin\", \"admin\"),\n",
    "    use_ssl=False,\n",
    ")\n",
    "\n",
    "info = client.info()\n",
    "print(f\"‚úÖ Connected to OpenSearch {info['version']['number']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646893f",
   "metadata": {},
   "source": [
    "#### **üß† Cell 3 ‚Äî Load Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddc9928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Embedding model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Load Embedding Model (same as index builder)\n",
    "# ------------------------------------------------------------\n",
    "embed_model = SentenceTransformer(MODEL_NAME)\n",
    "print(\"üß© Embedding model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197cdbc0",
   "metadata": {},
   "source": [
    "#### **==============================================================**\n",
    "#### **üí¨ 4Ô∏è‚É£ Load or Initialize Cache**\n",
    "#### **==============================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f57817f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Loaded 6 cached embeddings from: D:\\Patient Risk Prediction\\Patient-Risk-Prediction\\chatbot\\cache\\embeddings_cache.json\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(EMBED_CACHE_PATH):\n",
    "    try:\n",
    "        with open(EMBED_CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            cache = json.load(f)\n",
    "        print(f\"‚ö° Loaded {len(cache)} cached embeddings from: {EMBED_CACHE_PATH}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ö†Ô∏è Cache file was empty or corrupted. Starting fresh.\")\n",
    "        cache = {}\n",
    "else:\n",
    "    cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939016aa",
   "metadata": {},
   "source": [
    "#### **==============================================================**\n",
    "#### **üîç 5Ô∏è‚É£ Helper Functions**\n",
    "#### **==============================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee4571c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Loading lightweight model for CPU inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FLAN-T5 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def search_similar_docs(query, k=3):\n",
    "    \"\"\"\n",
    "    Performs a vector similarity search in OpenSearch.\n",
    "    \"\"\"\n",
    "    query_vector = embed_model.encode(query).tolist()\n",
    "\n",
    "    search_body = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"embedding\": {\n",
    "                    \"vector\": query_vector,\n",
    "                    \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = client.search(index=INDEX_NAME, body=search_body)\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    docs = [hit[\"_source\"][\"content\"] for hit in hits]\n",
    "    return docs\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load a small, efficient CPU-friendly model\n",
    "qa_model_name = \"google/flan-t5-base\"\n",
    "\n",
    "print(\"‚öôÔ∏è Loading lightweight model for CPU inference...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(qa_model_name)\n",
    "\n",
    "qa_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1  # Force CPU\n",
    ")\n",
    "print(\"‚úÖ FLAN-T5 model loaded successfully!\")\n",
    "\n",
    "def rag_answer(question, k=3):\n",
    "    \"\"\"\n",
    "    Enhanced RAG pipeline:\n",
    "    - Retrieves docs from OpenSearch for context.\n",
    "    - If the question sounds analytical, run live SQL from Databricks.\n",
    "    - Then generate a final summary answer.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve context\n",
    "    top_docs = search_similar_docs(question, k=k)\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "\n",
    "    # Step 2: Detect if the question needs SQL\n",
    "    sql_keywords = [\"average\", \"count\", \"total\", \"sum\", \"by\", \"show\", \"list\", \"how many\", \"compare\"]\n",
    "    if any(kw in question.lower() for kw in sql_keywords):\n",
    "        print(\"üßÆ Detected analytical intent ‚Äî generating SQL query...\")\n",
    "\n",
    "        # Ask your QA model to generate SQL\n",
    "        sql_prompt = f\"\"\"\n",
    "You are an expert data engineer.\n",
    "Given the question and context, generate an SQL query that can be run on the Databricks patient_risk_prediction database.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Return only the SQL query.\n",
    "\"\"\"\n",
    "        sql_query = qa_pipe(sql_prompt, max_new_tokens=150, truncation=True)[0][\"generated_text\"]\n",
    "        print(\"üßæ Generated SQL:\\n\", sql_query)\n",
    "\n",
    "        # Try running on Databricks\n",
    "        result = run_databricks_query(sql_query)\n",
    "        if result:\n",
    "            rows = result[\"rows\"]\n",
    "            cols = result[\"columns\"]\n",
    "            print(\"üìä Databricks Query Result:\")\n",
    "            print(cols)\n",
    "            print(rows[:5])  # preview top rows\n",
    "\n",
    "            # Summarize results\n",
    "            summary_prompt = f\"\"\"\n",
    "You are a data analyst. Summarize this Databricks SQL result clearly.\n",
    "\n",
    "Question: {question}\n",
    "Columns: {cols}\n",
    "Rows: {rows[:5]}\n",
    "\"\"\"\n",
    "            summary = qa_pipe(summary_prompt, max_new_tokens=100, truncation=True)[0][\"generated_text\"]\n",
    "            return summary\n",
    "\n",
    "    # Step 3: Default to text-only RAG answer\n",
    "    answer = generate_answer(question, context)\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rag_answer(question, k=3):\n",
    "    \"\"\"\n",
    "    Full RAG pipeline: retrieve -> generate -> answer\n",
    "    \"\"\"\n",
    "    top_docs = search_similar_docs(question, k=k)\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "    answer = generate_answer(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775c49a",
   "metadata": {},
   "source": [
    "üí¨ 6Ô∏è‚É£ Test the RAG Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29021c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer: vw_high_risk_patients.sql  config(materialized='view')  SELECT p.patient_sk, h.name, h.age, h.gender, h.medical_condition, h.hospital, h.insurance_provider, ROUND(h.billing_amount, 2) AS billing_amount, h.stay_duration_days, h.date_of_admission, h.discharge_date, CASE WHERE h.medical_condition IN ('Cancer', 'Heart Disease'\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 7Ô∏è‚É£ Test the RAG Chatbot\n",
    "# ------------------------------------------------------------\n",
    "question = \"What is the average billing amount by insurance company?\"\n",
    "answer = rag_answer(question)\n",
    "print(\"ü§ñ Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55406972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer: vw_high_risk_patients.sql  config(materialized='view')  SELECT p.patient_sk, h.name, h.age, h.gender, h.medical_condition, h.hospital, h.insurance_provider, ROUND(h.billing_amount, 2) AS billing_amount, h.stay_duration_days, h.date_of_admission, h.discharge_date, CASE WHERE h.medical_condition IN ('Cancer', 'Heart Disease'\n"
     ]
    }
   ],
   "source": [
    "question = \"Show total billing by insurance?\"\n",
    "answer = rag_answer(question)\n",
    "print(\"ü§ñ Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd76a49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer: patient_readmission_30d.sql  config( materialized='table', schema='ml')  WITH ordered AS ( SELECT name AS patient_name, gender, age, medical_condition, hospital, insurance_provider, date_of_admission, discharge_date, stay_duration_days, billing_amount, ROW_NUMBER() OVER ( PARTITION BY name ORDER BY date_of_admission ) AS encounter_id, LEAD(date_of_admission) OVER ( PARTITION BY name ORDER BY date_of_\n"
     ]
    }
   ],
   "source": [
    "question = \"How many patients were readmitted within 30 days?\"\n",
    "answer = rag_answer(question)\n",
    "print(\"ü§ñ Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1935fe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer: gold_objects_validation.sql USE patient_risk_prediction.gold; SHOW TABLES; select * from patient_risk_prediction.gold.dim_doctor; select * from patient_risk_prediction.gold.dim_patient; DESCRIBE patient_risk_prediction.gold.dim_patient; select count(*) from patient_risk_prediction.gold.dim_doctor; --50000 select * from patient_risk_prediction.gold.dim_doctor limit 10; select doctor_sk,count(*) from patient_risk_prediction.gold.d\n"
     ]
    }
   ],
   "source": [
    "question = \"List all tables in the gold layer.\"\n",
    "answer = rag_answer(question)\n",
    "print(\"ü§ñ Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
